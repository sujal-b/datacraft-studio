from celery import Celery
import pandas as pd
import numpy as np
import os
import json
from redis import Redis
from datetime import datetime, timezone
from scipy import stats
from statsmodels.tsa.stattools import acf
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from ai_service import get_ai_interpretation, get_treatment_plan_hypotheses
from data_type_detector import detect_data_type

class NumpyJSONEncoder(json.JSONEncoder):
    """
    A comprehensive JSON encoder that handles all common NumPy and pandas data types.
    This prevents TypeError exceptions for types like numpy.int64, numpy.float64, 
    pandas.Timestamp, etc.
    """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, pd.Timestamp):
            return obj.isoformat() # Convert Timestamp to standard ISO string
        return super(NumpyJSONEncoder, self).default(obj)


celery_app = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')
redis_cache = Redis(host='localhost', port=6379, db=1, decode_responses=True)

@celery_app.task(time_limit=900) # 15 minute time limit for huge files
def generate_comprehensive_stats(file_path: str):
    # This function remains unchanged.
    try:
        file_name = os.path.basename(file_path)
        cache_key = f"statistics:{file_name}"
        
        common_na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',
                            '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',
                            'nan', 'null', 'None']
        
        df = pd.read_csv(file_path, on_bad_lines='skip', na_values=common_na_values)

        if df.empty:
            redis_cache.delete(cache_key)
            return

        # --- Perform all calculations in one pass ---
        rows, columns = df.shape
        missing_cells = df.isnull().sum().sum()
        total_cells = rows * columns if rows > 0 else 1
        missing_pct = (missing_cells / total_cells) * 100
        duplicate_rows = df.duplicated().sum()
        duplicate_pct = (duplicate_rows / rows) * 100 if rows > 0 else 0
        quality_score = max(0, 100 - missing_pct - duplicate_pct)
        status = "RAW"
        if quality_score > 90: status = "CLEANED"
        elif quality_score > 60: status = "CLEANING"
        
        column_stats = []
        numeric_column_count = 0
        text_column_count = 0

        for header in df.columns:
            series = df[header]
            clean_series = series.dropna()
            null_count = int(series.isnull().sum())
            data_type = detect_data_type(series)

            if data_type in ['integer', 'float', 'identifier']:
                numeric_column_count += 1
            else:
                text_column_count += 1
            
            stat = {
                "column": header, "dataType": data_type, "nullCount": null_count,
                "nullPercentage": (null_count / rows) * 100 if rows > 0 else 0,
                "uniqueValues": series.nunique(), "totalValues": len(clean_series),
                "mean": "N/A", "median": "N/A", "mode": "N/A"
            }
            if data_type in ['integer', 'float'] and not clean_series.empty:
                stat["mean"] = round(clean_series.mean(), 2)
                stat["median"] = round(clean_series.median(), 2)
                modes = clean_series.mode()
                if not modes.empty:
                    stat["mode"] = ", ".join(modes.astype(str).tolist())
            column_stats.append(stat)

        comprehensive_result = {
            "filename": file_name,
            "lastModified": datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d'),
            "size": f"{os.path.getsize(file_path) / (1024*1024):.1f}MB",
            "rows": rows, "columns": columns, "totalCells": total_cells,
            "status": status, "qualityScore": round(quality_score),
            "missing_pct": round(missing_pct), "duplicates_pct": round(duplicate_pct),
            "overallNullCount": int(missing_cells),
            "columnStats": column_stats,
            "numericColumnCount": numeric_column_count,
            "textColumnCount": text_column_count
        }

        redis_cache.set(cache_key, json.dumps(comprehensive_result, cls=NumpyJSONEncoder), ex=86400)
        return comprehensive_result
    except Exception as e:
        print(f"CRITICAL ERROR in generate_comprehensive_stats for {file_path}: {e}")
        raise e

@celery_app.task(time_limit=1800)
def generate_diagnostic_report(file_path: str):
    """
    Reads a file ONCE and generates a single, comprehensive, and detailed
    Diagnostic Report with metrics optimized for LLM-powered data cleaning and imputation.
    """
    try:
        file_name = os.path.basename(file_path)
        cache_key = f"diagnostics:{file_name}"

        df = pd.read_csv(
            file_path,
            on_bad_lines='skip',
            na_values=['', 'NA', 'N/A', 'NULL', 'None', 'nan', 'NaN'],
            encoding='utf-8'
        )
        if df.empty:
            redis_cache.delete(cache_key)
            return {"status": "ERROR", "message": "Dataset is empty."}

        rows, columns = df.shape
        duplicate_row_count = int(df.duplicated().sum())
        total_missing_cells = int(df.isnull().sum().sum())
        total_cells = rows * columns
        overall_missing_percentage = (
            (total_missing_cells / total_cells) * 100 if total_cells > 0 else 0
        )
        max_missing_column_percentage = round(df.isnull().mean().max() * 100, 2)
        rows_gt_50pct_nulls = int((df.isnull().mean(axis=1) > 0.5).sum())

        dataset_summary = {
            "row_count": rows,
            "column_count": columns,
            "duplicate_row_count": duplicate_row_count,
            "total_missing_cells": total_missing_cells,
            "overall_missing_percentage": round(overall_missing_percentage, 2),
            "max_missing_column_percentage": max_missing_column_percentage,
            "rows_gt_50pct_nulls": rows_gt_50pct_nulls,
        }

        column_diagnostics = []
        for header in df.columns:
            series = df[header]
            clean_series = series.dropna()
            data_type = detect_data_type(series)
            unique_count = series.nunique(dropna=True)
            unique_ratio = round(unique_count / rows, 4) if rows else 0
            constant_flag = unique_count == 1

            col_diag = {
                "column_name": header,
                "data_type": data_type,
                "missing_count": int(series.isnull().sum()),
                "missing_percentage": round(series.isnull().mean() * 100, 2),
                "unique_count": int(unique_count),
                "unique_ratio": unique_ratio,
                "constant_flag": bool(constant_flag) # Defensive bool cast
            }

            if data_type in ['integer', 'float'] and not clean_series.empty:
                q1, q3 = clean_series.quantile(0.25), clean_series.quantile(0.75)
                iqr = q3 - q1
                outlier_count = (
                    ((clean_series < (q1 - 1.5 * iqr)) | (clean_series > (q3 + 1.5 * iqr))).sum()
                )
                if clean_series.nunique() > 2:
                    skewness = round(float(clean_series.skew()), 2)
                    kurtosis = round(float(clean_series.kurtosis()), 2)
                else:
                    skewness, kurtosis = None, None

                col_diag["numeric_profile"] = {
                    "mean": round(float(clean_series.mean()), 2),
                    "median": round(float(clean_series.median()), 2),
                    "std_dev": round(float(clean_series.std()), 2),
                    "min": round(float(clean_series.min()), 2),
                    "max": round(float(clean_series.max()), 2),
                    "skewness": skewness,
                    "kurtosis": kurtosis,
                    "outlier_count": int(outlier_count),
                    "outlier_percentage": round((outlier_count / len(clean_series) * 100) if len(clean_series) > 0 else 0, 2)
                }

            elif data_type == "categorical" and not clean_series.empty:
                value_counts = clean_series.value_counts()
                top_5 = value_counts.head(5).to_dict()
                most_freq_pct = (
                    round((value_counts.iloc[0] / len(clean_series)) * 100, 2)
                    if not value_counts.empty
                    else 0
                )
                col_diag["categorical_profile"] = {
                    "top_5_categories": {str(k): int(v) for k, v in top_5.items()},
                    "most_frequent_category_percentage": most_freq_pct
                }

            elif data_type == "date" and not clean_series.empty:
                datetime_series = pd.to_datetime(clean_series, errors="coerce").dropna()
                if not datetime_series.empty:
                    col_diag["datetime_profile"] = {
                        "min_date": datetime_series.min(), # Let the encoder handle Timestamp
                        "max_date": datetime_series.max(), # Let the encoder handle Timestamp
                    }

            elif data_type == "text" and not clean_series.empty:
                lengths = clean_series.astype(str).str.len()
                col_diag["text_profile"] = {
                    "avg_length": round(float(lengths.mean()), 2),
                    "empty_string_count": int((clean_series == '').sum())
                }

            column_diagnostics.append(col_diag)

        diagnostic_report = {
            "filename": file_name,
            "dataset_summary": dataset_summary,
            "column_diagnostics": column_diagnostics,
        }
        
        redis_cache.set(cache_key, json.dumps(diagnostic_report, cls=NumpyJSONEncoder), ex=86400)
        
        return diagnostic_report

    except Exception as e:
        print(f"CRITICAL ERROR in generate_diagnostic_report: {e}")
        raise e

# --- The rest of the file (get_temporal_profile, route_task, etc.) remains unchanged. ---

@celery_app.task(time_limit=1800)
def generate_treatment_plans_task(dataset_name: str, target_variable: str, goal: str):
    """
    Generates three competing data cleaning plans by passing the diagnostic report to an LLM.
    """
    try:
        cache_key = f"diagnostics:{dataset_name}"
        report_str = redis_cache.get(cache_key)
        
        if not report_str:
            return {"status": "FAILURE", "error": f"Diagnostic report for {dataset_name} not found in cache."}
            
        diagnostic_report = json.loads(report_str)
        
        # Add context for the AI, which can be used in more advanced prompts later
        diagnostic_report['modeling_context'] = {
            'target_variable': target_variable,
            'goal': goal
        }

        plans = get_treatment_plan_hypotheses(diagnostic_report)

        if "error" in plans:
             return {"status": "FAILURE", "error": plans.get("details", "AI service failed to generate plans.")}

        return {"status": "SUCCESS", "result": plans}
        
    except Exception as e:
        print(f"CRITICAL ERROR in generate_treatment_plans_task for {dataset_name}: {e}")
        return {"status": "FAILURE", "error": str(e)}

def get_temporal_profile(df: pd.DataFrame, col: str) -> dict:
    time_cols = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower()]
    if not time_cols:
        return {"is_time_series": False}
    time_col = time_cols[0]
    try:
        ts_data = pd.to_datetime(df[time_col], errors='coerce').dropna()
        if ts_data.empty: return {"is_time_series": False}
    except Exception:
        return {"is_time_series": False}
    ts_series = df.set_index(time_col)[col].sort_index().dropna()
    acf_1 = None
    if ts_series.nunique() > 1 and len(ts_series) > 1:
        try:
            acf_1 = round(acf(ts_series, nlags=1, fft=False)[1], 2)
        except Exception:
            acf_1 = None
    return {"is_time_series": True, "temporal_stability_acf1": acf_1}

def get_mnar_indicators(df: pd.DataFrame, col: str) -> dict:
    correlations = {}
    missing_indicator = df[col].isnull().astype(int)
    for other_col in df.columns:
        if other_col == col: continue
        try:
            if pd.api.types.is_numeric_dtype(df[other_col]) and df[other_col].nunique() > 1:
                corr = missing_indicator.corr(df[other_col])
                if pd.notnull(corr) and abs(corr) > 0.3:
                    correlations[other_col] = round(corr, 2)
        except Exception:
            continue
    return correlations

def get_statistical_profile(df: pd.DataFrame, column_name: str) -> dict:
    detected_type = detect_data_type(df[column_name])

    missing_count = int(df[column_name].isnull().sum())
    total_count = len(df[column_name])
    missing_pct = (missing_count / total_count) * 100 if total_count > 0 else 0

    profile = {
        "column": column_name,
        "missing_count": missing_count, 
        "missing_pct": round(missing_pct, 4),
        "data_type": detected_type,
        "unique_values": df[column_name].nunique()
    }
    if detected_type in ['integer', 'float', 'identifier']:
        clean_data = df[column_name].dropna()
        if not clean_data.empty:
            profile["mean"] = round(clean_data.mean(), 2)
            profile["median"] = round(clean_data.median(), 2)
    profile["mnar_indicators"] = get_mnar_indicators(df, column_name)
    profile.update(get_temporal_profile(df, column_name))
    return profile

def perform_standardization(df: pd.DataFrame, column_name: str, method: str, file_path: str) -> dict:
    new_col_name = f"{column_name}_{method}_scaled"
    if new_col_name in df.columns:
        return {"status": "SKIPPED", "message": f"Column '{new_col_name}' already exists."}
    if not pd.api.types.is_numeric_dtype(df[column_name]):
        raise ValueError(f"Column '{column_name}' is not numeric.")
    if df[column_name].isnull().any():
        raise ValueError(f"Column '{column_name}' contains missing values. Impute first.")
    scaler = StandardScaler() if method == 'standard' else MinMaxScaler()
    df[new_col_name] = scaler.fit_transform(df[[column_name]].values.astype(np.float32))
    df.to_csv(file_path, index=False)
    q1 = float(df[column_name].quantile(0.25))
    q3 = float(df[column_name].quantile(0.75))
    audit_report = {
        "new_column_added": new_col_name, "message": f"Successfully created new column: '{new_col_name}'.",
        "method": method, "timestamp_utc": datetime.now(timezone.utc).isoformat(), "row_count_affected": len(df),
        "input_stats_robust": {"median": float(df[column_name].median()), "interquartile_range": q3 - q1}
    }
    return audit_report

def perform_delete_column(df: pd.DataFrame, column_name: str, file_path: str):
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' not found.")
    df.drop(columns=[column_name], inplace=True)
    df.to_csv(file_path, index=False)
    return {"message": f"Successfully deleted column '{column_name}' and updated the dataset."}

@celery_app.task
def perform_dataset_cleaning_task(file_path: str, action_type: str):
    try:
        if not os.path.exists(file_path):
            return {"status": "FAILURE", "error": "File not found."}

        df = pd.read_csv(file_path, on_bad_lines='skip')
        original_rows = len(df)

        if action_type == 'drop_na_rows':
            df.dropna(inplace=True)
            rows_affected = original_rows - len(df)
            message = f"Successfully dropped {rows_affected} rows with missing values."
        
        elif action_type == 'drop_duplicate_rows':
            df.drop_duplicates(inplace=True)
            rows_affected = original_rows - len(df)
            message = f"Successfully dropped {rows_affected} duplicate rows."
            
        else:
            return {"status": "FAILURE", "error": f"Unknown cleaning action: {action_type}"}

        # Overwrite the original file with the cleaned data
        df.to_csv(file_path, index=False)

        return {"status": "SUCCESS", "message": message, "rows_affected": rows_affected}

    except Exception as e:
        print(f"CRITICAL ERROR in perform_dataset_cleaning_task for {file_path}: {e}")
        return {"status": "FAILURE", "error": str(e)}
    
def perform_imputation(df: pd.DataFrame, column_name: str, method: str, value=None) -> dict:
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' not found.")

    original_missing_count = int(df[column_name].isnull().sum())
    if original_missing_count == 0:
        return {"message": "No missing values to impute.", "rows_affected": 0}

    if method == 'mean':
        fill_value = df[column_name].mean()
        df[column_name].fillna(fill_value, inplace=True)
    elif method == 'median':
        fill_value = df[column_name].median()
        df[column_name].fillna(fill_value, inplace=True)
    elif method == 'mode':
        fill_value = df[column_name].mode()[0]
        df[column_name].fillna(fill_value, inplace=True)
    elif method == 'constant':
        dtype = df[column_name].dtype
        try:
            fill_value = pd.Series([value]).astype(dtype).iloc[0]
        except (ValueError, TypeError):
            fill_value = value
        df[column_name].fillna(fill_value, inplace=True)
    else:
        raise ValueError(f"Invalid imputation method: {method}")

    return {"message": f"Successfully imputed {original_missing_count} missing values in '{column_name}'.", "rows_affected": original_missing_count}


@celery_app.task
def route_task(dataset_name: str, column_name: str, task_type: str, task_params: dict = None):
    try:
        file_path = os.path.join(os.path.dirname(__file__), '..', 'public', dataset_name)
        df = pd.read_csv(file_path)
        if task_type == 'diagnosis':
            profile = get_statistical_profile(df, column_name)
            result = get_ai_interpretation(profile)
            return {"status": "SUCCESS", "result": result}
        elif task_type == 'delete_column':
            result = perform_delete_column(df, column_name, file_path)
            return {"status": "SUCCESS", "result": result}
        elif task_type.startswith('impute_'):
            method = task_type.split('_')[1]
            custom_value = task_params.get('value') if task_params else None
            result = perform_imputation(df, column_name, method, value=custom_value)
            df.to_csv(file_path, index=False)
            return {"status": "SUCCESS", "result": result}
        elif task_type in ['standard_scale', 'minmax_scale']:
            method = 'standard' if task_type == 'standard_scale' else 'minmax'
            result = perform_standardization(df, column_name, method, file_path)
            return {"status": "SUCCESS", "result": result}
        else:
            return {"status": "ERROR", "message": "Unknown task type."}
    except Exception as e:
        return {"status": "FAILURE", "error": str(e)}